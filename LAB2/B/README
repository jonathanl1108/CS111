#NAME: Jonathan Liau
#EMAIL: jonathanliau747@gmail.com
#ID: 105182103

QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

When there are only one or two threads, I believe most of the cycles are
spent doing the the three sorted-list-operations, insert, length and 
deleting the element into to the lists. This explanation is reasonable, since
the number of threads are low, there are less conflict accessing or waiting for 
the lock to be release. 

On the other hand, in the case of high spin lock tests, most of the time of the cycle
would probably be spent waiting in the while loop for the lock to be released, since 
the number of threads are high, conflicts of accessing the same lock is more likely to occurred. 
Furthermore, in the case of high-thread mutex tests, most of the cycle time would be spent 
doing context switch with the similar reason. The above discussion can be observed in lab2b_1.png
where we see the throughput for both spin and mutex synchronization drops as the number of threads grows
, and that is because most of the cpu time are spent waiting to acquire for lock rather than performing
the sorted-list operations.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

According to profile.out generated by the google-pprof tool, 
the line 202, which is 
    while (__sync_lock_test_and_set(&is_slock_held[index], 1));
in the thread_worker secton spends most the cycle time. This result is 
as expected, because when the number of threads are high, they are more 
likely to exisit conflicts when the threads are acquiring the same spin
lock, and therefore most of them would be spending a lot of time in the while loop 
for the lock the be released that makes the operation expensive. 


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The average lock-wait time rise so dramatically are being observed in the previous examination. 
As the number of threads increased, more contending occurred for the same mutex lock, resulting 
most of the contending threads spending the cycle time waiting for other thread to release the lock, 
which also indicates less operations would be accomplished by each threads in their turn. Furthermore, 
completion time per operation rise less dramatically, because each threads are still making small progress 
in each cycle time, even though most of the time are being spent for watiting. 

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. 
Does this appear to be true in the above curves? If not, explain why not.

As the number of sub list increased the the performance of the synchronized methods should 
also increased, because more parallelism could be achieved with multithreading. However, in the 
last two graph, the curve started to decrease after the threads exceeds 8. An explanation is that
there exist a threshold for the number of threads to supply, once the threads grows over such threshold,
the waiting time for the lock to be released will make the throughput to drop as we observed in the second
graph.
According to the the last graph, a N-way partitioned list is not equivalent to the 
throughput of a single list with fewer (1/N) threads,points on the N-list curve do not agree with the prompted 
suggestion. 









